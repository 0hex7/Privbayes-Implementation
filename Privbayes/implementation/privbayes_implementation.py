# -*- coding: utf-8 -*-
"""Privbayes-implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lg_2rgXjz10igkeC5nu78v5jG0RO7ZO-
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /

# Commented out IPython magic to ensure Python compatibility.
# %ls

import os

!pip install autograd

#!pip install scikit-glpk

!pip install pyomo
!pyomo solve abstract1.py abstract1.dat --solver=glpk

pip install swig cython pandas

# Commented out IPython magic to ensure Python compatibility.
#%rm -rf /ektelo/
# %cd /

!git clone https://github.com/0hex7/ektelo.git

# Set the HOME environment variable to root directory '/'
os.environ['HOME'] = '/'

# Define environment variables
os.environ['EKTELO_HOME'] = '/ektelo'
os.environ['EKTELO_DATA'] = '/tmp/ektelo'
os.environ['PYTHON_HOME'] = '/PyEktelo'
os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + ':/ektelo'
os.environ['EKTELO_LOG_PATH'] = '/logs'
os.environ['EKTELO_LOG_LEVEL'] = 'DEBUG'

# Retrieve and create directories
ektelo_home = os.getenv('EKTELO_HOME')
ektelo_data = os.getenv('EKTELO_DATA')
python_home = os.getenv('PYTHON_HOME')
pythonpath = os.getenv('PYTHONPATH')
ektelo_log_path = os.getenv('EKTELO_LOG_PATH')
ektelo_log_level = os.getenv('EKTELO_LOG_LEVEL')

directories = [ektelo_home, ektelo_data, python_home, pythonpath, ektelo_log_path, ektelo_log_level]

for directory in directories:
    try:
        os.makedirs(directory, exist_ok=True)
        print(f"Directory '{directory}' created successfully.")
    except OSError as error:
        print(f"Directory creation failed for '{directory}': {error}")

!sudo apt-get install gfortran liblapack-dev libblas-dev
!sudo apt-get install libpq-dev python3-dev libncurses5-dev swig glpk

print(f"EKTELO_HOME: {ektelo_home}")
print(f"EKTELO_DATA: {ektelo_data}")
print(f"PYTHON_HOME: {python_home}")
print(f"PYTHONPATH: {pythonpath}")
print(f"EKTELO_LOG_PATH: {ektelo_log_path}")
print(f"EKTELO_LOG_LEVEL: {ektelo_log_level}")

pwd

#ektelo_home = os.getenv('EKTELO_HOME')
#%cd $ektelo_home

cd /

# Commented out IPython magic to ensure Python compatibility.
!python3 -m venv $python_home
!source $python_home/bin/activate
# %cd $ektelo_home
!pip install -r resources/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %mkdir -p $ektelo_data
!curl -k https://www.dpcomp.org/data/cps.csv > $ektelo_data/cps.csv
!curl -k https://www.dpcomp.org/data/stroke.csv > $ektelo_data/stroke.csv

# Commented out IPython magic to ensure Python compatibility.
# %cd $ektelo_home/ektelo/algorithm
!./setup.sh

# Commented out IPython magic to ensure Python compatibility.
# %cd $ektelo_home/ektelo/algorithm
# %ls
# %cd ahp
!./setup.sh

# Commented out IPython magic to ensure Python compatibility.
# %cd $ektelo_home/ektelo/algorithm
#%pwd
# %ls
# %cd privBayes/
#%ls
!./setup.sh

# Commented out IPython magic to ensure Python compatibility.
# %pwd
!source $python_home/bin/activate

!cd $ektelo_home
!nosetests

# Commented out IPython magic to ensure Python compatibility.
# %cd /

rm -rf privbayes-implementation/

!git clone https://github.com/0hex7/privbayes-implementation.git

# Commented out IPython magic to ensure Python compatibility.
# %cd privbayes-implementation/

!python3 setup.py install
!pip install kaleido python-multipart uvicorn fastapi
!pip install numpy pillow

# Commented out IPython magic to ensure Python compatibility.
# %pwd
# %cd /privbayes-implementation/Privbayes/
# %ls
# %cd data/
# %ls
# %cd ../code/

!python privbayes-4.py

"""=========================================================================================================





"""

from IPython.display import Image

Image(filename='/privbayes-implementation/Privbayes/data/graphs/comparision_graph_adult.png')

# Commented out IPython magic to ensure Python compatibility.
# %cd ../data/graphs/
# %ls
# %rm comparison_graph_adult_tiny.png

import os
import itertools
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

def comparedatasets(input_df, synthetic_df, file_name):
    original_columns = input_df.columns.tolist()
    synthetic_columns = synthetic_df.columns.tolist()
    print(original_columns)
    print(synthetic_columns)

    # Find common attributes between original and synthetic datasets
    common_attributes = set(original_columns) & set(synthetic_columns)

    # Generate attribute pairs for the common attributes and sort them for consistency
    attribute_pairs = sorted(list(itertools.combinations(common_attributes, 2)))

    # Define a minimum threshold for co-occurrence counts
    min_threshold = 20  # Adjust this threshold as needed

    # Create a single bar graph for co-occurrence counts of attribute pairs in both datasets
    fig, axs = plt.subplots(len(attribute_pairs), 1, figsize=(15, 5 * len(attribute_pairs)))

    for i, attribute_pair in enumerate(attribute_pairs):
        # Calculate the counts of occurrences for each unique pair of values in input_df
        input_attribute_counts = input_df.groupby(list(attribute_pair)).size().reset_index(name='Count')

        # Calculate the counts of occurrences for each unique pair of values in synthetic_df
        synthetic_attribute_counts = synthetic_df.groupby(list(attribute_pair)).size().reset_index(name='Count')

        # Filter insignificant co-occurrences based on the threshold
        input_attribute_counts = input_attribute_counts[input_attribute_counts['Count'] >= min_threshold]
        synthetic_attribute_counts = synthetic_attribute_counts[synthetic_attribute_counts['Count'] >= min_threshold]

        # Merge the co-occurrence counts for attribute pairs in both datasets
        merged_counts = pd.merge(input_attribute_counts, synthetic_attribute_counts, on=list(attribute_pair), how='outer', suffixes=('_original', '_synthetic'))
        merged_counts = merged_counts.fillna(0)  # Replace NaNs with 0s

        # Plotting the counts of co-occurrences as a single bar plot for both datasets
        bar_width = 0.35
        index = np.arange(len(merged_counts))
        original_bars = axs[i].bar(index, merged_counts['Count_original'], bar_width, label='Original', color='blue')
        synthetic_bars = axs[i].bar(index + bar_width, merged_counts['Count_synthetic'], bar_width, label='Synthetic', color='orange')
        axs[i].set_xlabel(f"{attribute_pair[0]} - {attribute_pair[1]}")
        axs[i].set_ylabel('Count')
        axs[i].set_title(f"Co-occurrence of {attribute_pair[0]} and {attribute_pair[1]}")
        axs[i].set_xticks(index + bar_width / 2)
        axs[i].set_xticklabels(merged_counts.apply(lambda x: f"{x[attribute_pair[0]]} - {x[attribute_pair[1]]}", axis=1), rotation=90)
        axs[i].legend()

    plt.tight_layout()

    # Directory to save the graphs
    save_dir = '/privbayes-implementation/Privbayes/data/graphs/'

    # Check if the directory exists, if not, create it
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # Save the graph
    save_path = os.path.join(save_dir, f"comparison_graph_{file_name}.png")
    plt.savefig(save_path)
    plt.close()

    return save_path


file_name = 'adult_tiny'
synthetic_df = f'/privbayes-implementation/Privbayes/data/synthetic-output/preprocessed_synthetic_{file_name}.csv'
input_df = f'/privbayes-implementation/Privbayes/data/preprocessed-output/preprocessed_{file_name}.csv'
original_dataset = f'/privbayes-implementation/Privbayes/data/{file_name}.csv'
final_synthetic_data = f'/privbayes-implementation/Privbayes/data/postprocessed-output/final_synthetic_{file_name}.csv'

    # Load the original dataset before preprocessing and display its head
original_data_before_preprocess = pd.read_csv(original_dataset)


    # Load the preprocessed dataset and display its head
preprocessed_data = pd.read_csv(input_df)


    # Load the synthetic dataset and display its head
synthetic_data = pd.read_csv(synthetic_df)


    # Display the head of the original synthetic dataset (postprocessed synthetic dataset)
final_synthetic_dataset = pd.read_csv(final_synthetic_data)


image_path = comparedatasets(original_data_before_preprocess, final_synthetic_dataset, file_name)
print(image_path)

from IPython.display import Image

Image(filename='/privbayes-implementation/Privbayes/data/graphs/comparison_graph_adult_tiny.png')

import pandas as pd
import json
import os
import numpy as np

# Function to preprocess the original dataset and generate domain values
def preprocess(original_dataset):
    # Load your dataset
    data = pd.read_csv(original_dataset)

    # Extract file name from the path (without extension)
    file_name = os.path.splitext(os.path.basename(original_dataset))[0]

    # Output directory path
    output_directory = '/temporary/'

    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Output file names
    processed_output_file = output_directory + f'processed_{file_name}.csv'
    domain_output_file = output_directory + f'domain_{file_name}.json'

    # Check if processed output file exists, if yes, remove it
    if os.path.isfile(processed_output_file):
        os.remove(processed_output_file)

    # Check if domain output file exists, if yes, remove it
    if os.path.isfile(domain_output_file):
        os.remove(domain_output_file)

    # Initialize an empty dictionary to store domain values for each column
    domain_values = {}

    # Categorize columns and generate domain values
    processed_data = data.copy()
    for column in data.columns:
        # Create a mapping of unique categorical values to numeric labels
        unique_values = data[column].unique()
        value_mapping = {val: idx + 1 for idx, val in enumerate(unique_values)}

        # Map categorical values to numeric labels in the dataset
        processed_data[column] = processed_data[column].map(value_mapping)

        # Convert int64 values to native int before storing in domain_values
        unique_values = [val.item() if isinstance(val, np.int64) else val for val in unique_values]

        # Store the mapping information in the domain_values dictionary
        domain_values[column] = {str(idx + 1): val for idx, val in enumerate(unique_values)}

    # Save the processed dataset with categorization to the output directory
    processed_data.to_csv(processed_output_file, index=False)

    # Save domain values as a JSON file in the output directory
    with open(domain_output_file, 'w') as json_file:
        json.dump(domain_values, json_file)

    # Return the processed dataset file path and domain file path
    return processed_output_file, domain_output_file, file_name

# Function to convert processed data back to original form after postprocessing
def postprocess(processed_input_dataset, domain_file):
    # Load processed input dataset
    processed_data = pd.read_csv(processed_input_dataset)

    # Load domain values for both input and synthetic datasets
    with open(domain_file, 'r') as json_file:
        domain_values = json.load(json_file)

    # Function to convert processed data back to original form
    def convert_to_original(processed_data, domain_values):
        for col in processed_data.columns:
            if col in domain_values:
                # Map numerical representation back to original categorical values using domain_values
                processed_data[col] = processed_data[col].astype(str).map(domain_values[col])

        return processed_data

    # Use the function to convert processed data back to its original form
    original_data = convert_to_original(processed_data, domain_values)

    # Save the processed data after postprocessing
    output_file = 'final_original_data.csv'  # Define the output file path
    original_data.to_csv(output_file, index=False)
    print(f"\nProcessed data after postprocessing is saved to: {output_file}")

    # Display head of the processed dataset after postprocessing
    print("\nHead of the processed dataset after postprocessing:")
    print(original_data.head())

# Replace 'original_dataset.csv' with your original dataset file
original_dataset_file = '/privbayes-implementation/Privbayes/data/adult_tiny.csv'
processed_output_file, domain_output_file, file_name = preprocess(original_dataset_file)

# Load the preprocessed dataset and display its head
preprocessed_data = pd.read_csv(processed_output_file)
print("\nHead of the preprocessed dataset:")
print(preprocessed_data.head())


# Load domain values from the JSON file
with open(domain_output_file, 'r') as json_file:
    domain_values = json.load(json_file)

# Print the loaded domain values
print(domain_values)


# Call the postprocess function to convert processed data back to its original form
postprocess(processed_output_file, domain_output_file)

import json

# Path to the domain file
domain_file_path = '/privbayes-implementation/Privbayes/data/processed-output/domain_adult_tiny.json'

# Read the content of the domain file
with open(domain_file_path, 'r') as file:
    domain_content = json.load(file)

# Display the content of the domain file
print(domain_content)

import pandas as pd
import json
import os

def postprocess(processed_input_dataset, processed_synthetic_dataset, domain_file):
    def convert_to_original(processed_data, domain_values):
        reverse_mappings = {}
        for col, values in domain_values.items():
            if isinstance(values, int):
                reverse_mappings[col] = {str(i): i for i in range(values)}
            else:
                reverse_mappings[col] = {str(v): k for k, v in values.items()}

        for col in processed_data.columns:
            if col in reverse_mappings:
                processed_data[col] = processed_data[col].astype(str).map(reverse_mappings[col])

        return processed_data

    def save_final_original(processed_data, output_file_path):
        processed_data.to_csv(output_file_path, index=False)

    # Load processed input dataset
    processed_input_data = pd.read_csv(processed_input_dataset)

    # Load processed synthetic dataset
    processed_synthetic_data = pd.read_csv(processed_synthetic_dataset)

    # Load domain values for both input and synthetic datasets
    with open(domain_file, 'r') as json_file:
        domain_values = json.load(json_file)

    # Convert processed input data to original format
    original_input_data = convert_to_original(processed_input_data.copy(), domain_values)
    original_synthetic_data = convert_to_original(processed_synthetic_data.copy(), domain_values)

    # Extract dataset names from the processed dataset file paths
    input_dataset_name = os.path.splitext(os.path.basename(processed_input_dataset))[0][10:]  # Remove 'processed_' from file name
    synthetic_dataset_name = os.path.splitext(os.path.basename(processed_synthetic_dataset))[0][10:]  # Remove 'processed_' from file name

    # Output directory path
    output_directory = '/privbayes-implementation/Privbayes/data/final_output'

    # Create the output directory if it doesn't exist
    os.makedirs(output_directory, exist_ok=True)

    # Output file names for final original datasets
    final_original_input_output_file = os.path.join(output_directory, f'final_original_{input_dataset_name}.csv')
    final_original_synthetic_output_file = os.path.join(output_directory, f'final_synthetic_{synthetic_dataset_name}.csv')

    # Save final original datasets without categorized data
    save_final_original(original_input_data, final_original_input_output_file)
    save_final_original(original_synthetic_data, final_original_synthetic_output_file)

    return final_original_input_output_file, final_original_synthetic_output_file

# Adjust file paths for your dataset
processed_input_dataset = '/privbayes-implementation/Privbayes/data/processed-output/processed_adult_tiny.csv'
processed_synthetic_dataset = '/privbayes-implementation/Privbayes/data/processed-output/synthetic_adult_tiny.csv'
domain_file = '/privbayes-implementation/Privbayes/data/processed-output/domain_adult_tiny.json'

final_original_input_file, final_original_synthetic_file = postprocess(processed_input_dataset, processed_synthetic_dataset, domain_file)

print(f"Final original input dataset without categorized data: {final_original_input_file}")
print(f"Final original synthetic dataset without categorized data: {final_original_synthetic_file}")

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %cd ../final_output/
# %ls
data = pd.read_csv('final_synthetic_adult_tiny.csv')
print(data)

from google.colab import files

# File paths of the final original and synthetic datasets
final_original_input_file = '/privbayes-implementation/Privbayes/data/final_output/final_original_adult_tiny.csv'
final_original_synthetic_file = '/privbayes-implementation/Privbayes/data/final_output/final_synthetic_adult_tiny.csv'

# Download final original input dataset
files.download(final_original_input_file)

# Download final synthetic dataset
files.download(final_original_synthetic_file)

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %cd synthetic-output/
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %pwd

import itertools
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files

# Prompt the user to upload the input dataset
print("Please upload the input dataset:")
uploaded = files.upload()

# Read the uploaded input dataset into a pandas DataFrame
input_df = pd.read_csv(next(iter(uploaded)))

# Prompt the user to upload the synthetic dataset
print("\nPlease upload the synthetic dataset:")
uploaded = files.upload()

# Read the uploaded synthetic dataset into a pandas DataFrame
synthetic_df = pd.read_csv(next(iter(uploaded)))

# Get columns (attributes) from both original and synthetic datasets
original_columns = input_df.columns.tolist()
synthetic_columns = synthetic_df.columns.tolist()

# Find common attributes between original and synthetic datasets
common_attributes = set(original_columns) & set(synthetic_columns)

# Generate attribute pairs for the common attributes
attribute_pairs = list(itertools.combinations(common_attributes, 2))

# Define a minimum threshold for co-occurrence counts
min_threshold = 30  # Adjust this threshold as needed

# Create subplots for co-occurrence graphs for each attribute pair
fig, axs = plt.subplots(len(attribute_pairs), 2, figsize=(15, 5 * len(attribute_pairs)))

for i, attribute_pair in enumerate(attribute_pairs):
    # Calculate the counts of occurrences for each unique pair of values in input_df
    input_attribute_counts = input_df.groupby(list(attribute_pair)).size().reset_index(name='Count')

    # Calculate the counts of occurrences for each unique pair of values in synthetic_df
    synthetic_attribute_counts = synthetic_df.groupby(list(attribute_pair)).size().reset_index(name='Count')

    # Filter insignificant co-occurrences based on the threshold
    input_attribute_counts = input_attribute_counts[input_attribute_counts['Count'] >= min_threshold]
    synthetic_attribute_counts = synthetic_attribute_counts[synthetic_attribute_counts['Count'] >= min_threshold]

    # Update the maximum counts within the current attribute pair for input and synthetic datasets
    max_input_count = input_attribute_counts['Count'].max() if not input_attribute_counts.empty else 0
    max_synthetic_count = synthetic_attribute_counts['Count'].max() if not synthetic_attribute_counts.empty else 0

    # Plotting the counts of co-occurrences as bar plots for input_df and synthetic_df
    input_plot = axs[i, 0].bar(input_attribute_counts.apply(lambda x: f"{x[attribute_pair[0]]} - {x[attribute_pair[1]]}", axis=1), input_attribute_counts['Count'])
    axs[i, 0].set_xlabel(f"{attribute_pair[0]} - {attribute_pair[1]}")
    axs[i, 0].set_ylabel('Count')
    axs[i, 0].set_title(f"Co-occurrence of {attribute_pair[0]} and {attribute_pair[1]} in input_df")
    axs[i, 0].tick_params(axis='x', rotation=90)
    axs[i, 0].set_ylim(0, max_input_count * 1.1)  # Set y-axis limits based on input_df counts

    synthetic_plot = axs[i, 1].bar(synthetic_attribute_counts.apply(lambda x: f"{x[attribute_pair[0]]} - {x[attribute_pair[1]]}", axis=1), synthetic_attribute_counts['Count'])
    axs[i, 1].set_xlabel(f"{attribute_pair[0]} - {attribute_pair[1]}")
    axs[i, 1].set_ylabel('Count')
    axs[i, 1].set_title(f"Co-occurrence of {attribute_pair[0]} and {attribute_pair[1]} in synthetic_df")
    axs[i, 1].tick_params(axis='x', rotation=90)
    axs[i, 1].set_ylim(0, max_synthetic_count * 1.1)  # Set y-axis limits based on synthetic_df counts

plt.tight_layout()
plt.show()

